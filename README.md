# Project: Microarray Leukemia Classification

### Overview
Classified microarray samples as Acute Lymphoblastic Leukemia (ALL) or Acute Myeloid Leukemia (AML) using a support vector classifier (SVC) on a small subset of the normalized intensity values of 7,129 genes from microarray samples generated by Golub, et al.
Explored effect of data preprocessing on final model, resulting in models with accuracy > 85% on test set.

### Some Details

I explored 2 options for preprocessing the data:
- no additional pre-processing
- zero cutoff for gene expression, log base 2 transformed (file names appended with 'log2')

To develop a method for feature selection, I reduced 7,129 gene features to ~5 features for classification by:
- generating metrics on each gene/feature (i.e. t-test p-values, mutual information scores) 
- filtering features based on chosen threshold values for metrics
- generating correlation matrix on remaining features
- eliminating features correlated above threshold, keeping features with lower p-value or mutual information metric

File overview:
- *microarray_formatcsv.ipynb*
>I reformat the data to separate metadata and eliminate unused data columns. I confirm that the classes of the training and test data sets are relatively balanced.

- *microarray_explore.ipynb* and *microarray_explorelog2.ipynb*
>I explore metrics that could be used to filter the features and estimate the percentage of differentially expressed genes between the classes.

- *microarray_model_adjp.ipynb* and *microarray_model_adjplog2.ipynb*
>I establish a baseline score for a suport vector classifier using all the features for classification. To improve the score and explore how few features are needed to separate the classes, I filter the features. In the feature reduction, I calculate multiple metrics on the data and eliminate features that don't meet set thresholds with a goal of severely reducing the feature set. On the final feature set, I rerun the support vector classifier to see the score improvement.

- *microarray_modelanalysis.ipynb* and *microarray_modelanalysis_log2.ipynb*
>I look at the distribution of scores for support vector classifiers using using a small set of randomly chosen features from the original feature set. I also explore the score distribution of classifiers using randomly chosen features from the reduced feature sets that result from the feature reduction process.



### Observations
- The histogram of p-values for each gene indicates a high number of differentially expressed genes.
- Estimated ~50% of genes differentially expressed.
- Simulation of picking a random 5 genes for classification showed a mean classification score of ~0.62.

When data log transformed with zero cutoff for expression (files appended log2):
- Adjusted p-values obtained by permutation of the samples resulted in a slightly lower number of genes with low p-values, but the overall distribution appeared unchanged.
- Although a final feature set of ~5 genes was chosen, these may not have been the best features. A simulation of randomly picking 5 genes from the filtered gene set (before eliminating genes based on correlation) shows that another set of 5 genes could have better classified the test data.


When data used without additional preprocessing:
- Adjusted p-values obtained by permutation of the samples resulted in a slightly larger number of genes with low p-values, but the overall distribution appeared unchanged.
- Although a final feature set of ~5 genes was chosen, the classification results were comparable when some genes in the final feature set were substituted with 
highly correlated alternate genes (those eliminated in the final step of feature selection due to high correlation). 
- Since many genes are differentially expressed between the ALL and AML groups, the genes in the final model appear to be interchangable with multiple other 
differentially expressed genes for classification.

### Language
Python

### Packages Used
numpy, pandas, scipy, sklearn, matplotlib, seaborn

### Model
Support Vector Classifier

### Resources
[Kaggle](https://www.kaggle.com/)
